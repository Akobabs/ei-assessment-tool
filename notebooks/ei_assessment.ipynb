{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee034c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# EI Assessment Tool: Data Preprocessing, Training, and Evaluation\n",
    "# Requires: pandas, numpy, nltk, transformers, scikit-learn, torch, datasets\n",
    "# Dataset: GoEmotions (place goemotions_1.csv, goemotions_2.csv in ./data/)\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load dataset\n",
    "train_df = pd.read_csv('data/goemotions_1.csv')\n",
    "test_df = pd.read_csv('data/goemotions_2.csv')\n",
    "\n",
    "# Text cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()\n",
    "    text = ' '.join(word for word in word_tokenize(text) if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Map emotions to EI attributes\n",
    "emotion_map = {\n",
    "    'caring': 'empathy', 'gratitude': 'empathy', 'approval': 'empathy', 'admiration': 'empathy',\n",
    "    'confusion': 'self_awareness', 'nervousness': 'self_awareness', 'realization': 'self_awareness',\n",
    "    'amusement': 'social_skills', 'curiosity': 'social_skills', 'excitement': 'social_skills'\n",
    "}\n",
    "\n",
    "def map_to_ei(emotions):\n",
    "    emotions = emotions.split(',')\n",
    "    for e in emotions:\n",
    "        if e in emotion_map:\n",
    "            return emotion_map[e]\n",
    "    return 'other'\n",
    "\n",
    "train_df['ei_label'] = train_df['emotions'].apply(map_to_ei)\n",
    "test_df['ei_label'] = test_df['emotions'].apply(map_to_ei)\n",
    "\n",
    "# Tokenization for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_df['clean_text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_df['clean_text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df['ei_label'])\n",
    "y_test = le.transform(test_df['ei_label'])\n",
    "\n",
    "# Prepare datasets for BERT\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': y_train\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'labels': y_test\n",
    "})\n",
    "\n",
    "# Train BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(le.classes_))\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Logistic Regression baseline\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', max_length=128, truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :].numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "X_train = get_bert_embeddings(train_df['clean_text'].tolist())\n",
    "X_test = get_bert_embeddings(test_df['clean_text'].tolist())\n",
    "lr_model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "bert_predictions = trainer.predict(test_dataset).predictions.argmax(-1)\n",
    "bert_scores = trainer.predict(test_dataset).predictions\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "lr_scores = lr_model.predict_proba(X_test)\n",
    "\n",
    "print(\"BERT Model:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, bert_predictions):.2f}\")\n",
    "print(f\"Precision: {precision_recall_fscore_support(y_test, bert_predictions, average='weighted')[0]:.2f}\")\n",
    "print(f\"Recall: {precision_recall_fscore_support(y_test, bert_predictions, average='weighted')[1]:.2f}\")\n",
    "print(f\"F1 Score: {precision_recall_fscore_support(y_test, bert_predictions, average='weighted')[2]:.2f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, bert_scores, multi_class='ovr'):.2f}\")\n",
    "\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, lr_predictions):.2f}\")\n",
    "print(f\"Precision: {precision_recall_fscore_support(y_test, lr_predictions, average='weighted')[0]:.2f}\")\n",
    "print(f\"Recall: {precision_recall_fscore_support(y_test, lr_predictions, average='weighted')[1]:.2f}\")\n",
    "print(f\"F1 Score: {precision_recall_fscore_support(y_test, lr_predictions, average='weighted')[2]:.2f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, lr_scores, multi_class='ovr'):.2f}\")\n",
    "\n",
    "# Save BERT model\n",
    "model.save_pretrained('models/bert_ei')\n",
    "tokenizer.save_pretrained('models/bert_ei')\n",
    "import joblib\n",
    "joblib.dump(lr_model, 'models/lr_ei.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
